if(!require("osmdata")) install.packages("osmdata")
library(osmdata)
citation ("osmdata")
#The sf we will need to make geometrical calculations.
if(!require("sf")) install.packages("sf")
library(sf)
#Further we need to access tidyverse.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
city="Berlin"
Brandenburg_Gate=c(13.377336846520663,52.516264818429924)
#As second we build a query asking for traffic signals in Berlin.
q <- getbb(city) %>%
opq() %>%
add_osm_feature("highway", "traffic_signals")
#Read the osm data format as a list in R.
signals <- osmdata_sf(q)
#If you access signals:
signals
distances=c(1:length(signals$osm_points$osm_id))
for(i in 1:length(distances)){
distances[i]=distm(Brandenburg_Gate, c(signals$osm_points$geometry[[i]][1],signals$osm_points$geometry[[i]][2]), fun=distGeo)
}
#This is a script for a tutorial
#You can learn to get the coordinates of points of interested by collecting data via open street map.
#For that purpose we will use the osmdata package.
if(!require("osmdata")) install.packages("osmdata")
library(osmdata)
#Do not forget to give credit to the creators.
citation ("osmdata")
#The sf we will need to make geometrical calculations.
if(!require("sf")) install.packages("sf")
library(sf)
#Further we need to access tidyverse.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
if(!require("geosphere")) install.packages("geosphere")
library(geosphere)#package for calculating distance using longitude and latitude
citation ("osmdata")
#The sf we will need to make geometrical calculations.
if(!require("sf")) install.packages("sf")
library(sf)
#Further we need to access tidyverse.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
if(!require("geosphere")) install.packages("geosphere")
library(geosphere)#package for calculating distance using longitude and latitude
#First we determine which city we want to study.
city="Berlin"
#Or you use another data source.
Brandenburg_Gate=c(13.377336846520663,52.516264818429924)
q <- getbb(city) %>%
opq() %>%
add_osm_feature("highway", "traffic_signals")
signals <- osmdata_sf(q)
signals
signals$osm_points$geometry
distances=c(1:length(signals$osm_points$osm_id))
for(i in 1:length(distances)){
distances[i]=distm(Brandenburg_Gate, c(signals$osm_points$geometry[[i]][1],signals$osm_points$geometry[[i]][2]), fun=distGeo)
}
min(distances)
sum(distances < 1000)
q2 <- getbb(city) %>%
opq() %>%
add_osm_feature("highway", "primary")
primary <- osmdata_sf(q2)
#Since now we are handeling street, we are not longer interested in osm_points but osm_lines
Lines_primary = st_transform(primary$osm_lines$geometry,4269)
#We need to convert our point at Brandenburg Gate to another data format
POINT_Brandenburg_Gate = as.data.frame(rbind(Brandenburg_Gate))
names(POINT_Brandenburg_Gate)[1]="long1"
names(POINT_Brandenburg_Gate)[2]="lat1"
POINT_Brandenburg_Gate = st_as_sf(POINT_Brandenburg_Gate, coords = c("long1","lat1"))
POINT_Brandenburg_Gate <- st_set_crs(POINT_Brandenburg_Gate, 4269)
#now we can use the st_distance() function to calculate each distance from our Point to each line in our primary street network.
#The smallest distance is:
min(st_distance(POINT_Brandenburg_Gate$geometry, Lines_primary))
library(lubridate)
library(dplyr)
library(geosphere)#package for calculating distance using longitude and latitude
#Clean up memory
rm(list=ls())
#Source storage location (outside the GitHub Repository)
#Because of file size limitation
#files about 100 MB have to be excluded
#D:\STUDIUM\MÃ¼nster\7. Semester\Masterarbeit Daten\Bochum
setwd("D:/STUDIUM/MÃ¼nster/7. Semester/Masterarbeit Daten/Darmstadt")
#Clean up memory
rm(list=ls())
setwd("D:/ComicandSonsProductions/GameJam1/SpeakerRecognition/TestData")
mapData = read.csv(file = "Test.csv",sep=",")
mapData
VoiceData = read.csv(file = "Test.csv",sep=".")
VoiceData = read.csv(file = "Test.csv",sep=";")
VoiceData
View(VoiceData)
#Clean up memory
rm(list=ls())
#Load Data
setwd("D:/ComicandSonsProductions/GameJam1/SpeakerRecognition/TestData")
VoiceData = read.csv(file = "Test.csv",sep=",")
View(VoiceData)
#In order to create a SVR model with R you will need the package e1071
if(!require("e1071")) install.packages("e1071")
library(e1071)
#In order to make a notification sound to inform the user that calculations are finished
if(!require("beepr")) install.packages("beepr")
library(beepr)
library(tidyverse)
library(sandwich)
library(caret)
VoiceData
VoiceData[1]
VoiceData[,1]
VoiceData[,2]
names(VoiceData)
names(VoiceData[,2])
names(VoiceData[2])
names(VoiceData[1])
VoiceData[2]
VoiceData[1]=NULL
names(VoiceData[1])="Voice"
VoiceData
VoiceData
names(VoiceData[1])
names(VoiceData)[1]="Voice"
VoiceData
# Split data to reduce duration of computation
training.samples <- VoiceData$Voice %>%
createDataPartition(p = 0.2, list = FALSE)
train.data  <- trainSet[training.samples, ]
# Split data to reduce duration of computation
training.samples <- VoiceData$Voice %>%
createDataPartition(p = 0.2, list = FALSE)
train.data  <- VoiceData[training.samples, ]
test.data <- VoiceData[-training.samples, ]
nrow(VoiceData)
model <- svm(log(Voice) ~ , data =  train.data, type = "C-classification", kernel = "polynomial", epsilon = 0.1)
model <- svm(Voice ~ , data =  train.data, type = "C-classification", kernel = "polynomial", epsilon = 0.1)
model <- svm(Voice ~., data =  train.data, type = "C-classification", kernel = "polynomial", epsilon = 0.1)
class(Voice$Data$Voice)
class(VoiceData$Voice)
as.factor(VoiceData$Voice)
VoiceData$Voice = as.factor(VoiceData$Voice)
# Split data to reduce duration of computation
training.samples <- VoiceData$Voice %>%
createDataPartition(p = 0.2, list = FALSE)
train.data  <- VoiceData[training.samples, ]
test.data <- VoiceData[-training.samples, ]
model <- svm(Voice ~., data =  train.data, type = "C-classification", kernel = "polynomial", epsilon = 0.1)
test_predict <- as.data.frame(predict(model, newdata = test.data, type='response'))
train_predict <- as.data.frame(predict(model, data = train.data, type='response'))
Test_RMSE = sqrt(mean((test.data$Value - exp(test_predict[,1]))^2))
Train_RMSE = sqrt(mean((train.data$Value - exp(train_predict[,1]))^2))
Test_R = postResample(exp(test_predict), test.data$Value)[2]
Train_R = postResample(exp(train_predict), train.data$Value)[2]
test_predict
train_predict
test.data$Voice
test_predict
data<-data.frame(test.data$Voice,test_predict)
data
names(data)<-c("yes","no")
data
install.packages("verification")
library(verification)
roc.plot(data$yes, data$no)
data<-data.frame(test.data$Voice,test_predict)
data
EvaluationData<-data.frame(test.data$Voice,test_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Prediction == EvaluationData$Observation
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
EvaluationData
sum(EvaluationData$Prediction == EvaluationData$Observation)
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
EvaluationData<-data.frame(train.data$Voice,train_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
EvaluationData
#Clean up memory
rm(list=ls())
#Load Data
setwd("D:/ComicandSonsProductions/GameJam1/SpeakerRecognition/TestData")
VoiceData = read.csv(file = "Test.csv",sep=",")
VoiceData[1]=NULL
names(VoiceData)[1]="Voice"
class(VoiceData$Voice)
VoiceData$Voice = as.factor(VoiceData$Voice)
nrow(VoiceData)
# Split data to reduce duration of computation
training.samples <- VoiceData$Voice %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- VoiceData[training.samples, ]
test.data <- VoiceData[-training.samples, ]
model <- svm(Voice ~., data =  train.data, type = "C-classification", kernel = "polynomial", epsilon = 0.1)
test_predict <- as.data.frame(predict(model, newdata = test.data, type='response'))
train_predict <- as.data.frame(predict(model, data = train.data, type='response'))
EvaluationData<-data.frame(test.data$Voice,test_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
EvaluationData<-data.frame(train.data$Voice,train_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
EvaluationData
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
EvaluationData<-data.frame(test.data$Voice,test_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
EvaluationData<-data.frame(train.data$Voice,train_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
if(!require("beepr")) install.packages("beepr")
if(!require("Rcpp")) install.packages("Rcpp")
library(randomForest)
library(Rcpp)
library(tidyverse)
library(sandwich)
library(caret)
#Clean up memory
rm(list=ls())
#Load Data
setwd("D:/ComicandSonsProductions/GameJam1/SpeakerRecognition/TestData")
VoiceData = read.csv(file = "Test.csv",sep=",")
VoiceData[1]=NULL
names(VoiceData)[1]="Voice"
class(VoiceData$Voice)
VoiceData$Voice = as.factor(VoiceData$Voice)
nrow(VoiceData)
# Split data to reduce duration of computation
training.samples <- VoiceData$Voice %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- VoiceData[training.samples, ]
test.data <- VoiceData[-training.samples, ]
model <- randomForest(Voice ~., data =  train.data, ntree=500, importance=TRUE)
test_predict <- as.data.frame(predict(model, newdata = test.data, type='response'))
train_predict <- as.data.frame(predict(model, data = train.data, type='response'))
EvaluationData<-data.frame(test.data$Voice,test_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
EvaluationData<-data.frame(train.data$Voice,train_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
EvaluationData
if(!require("neuralnet")) install.packages("neuralnet")
library(neuralnet)
if(!require("MASS")) install.packages("MASS")
library(MASS)
library(tidyverse)
library(sandwich)
library(caret)
#Clean up memory
rm(list=ls())
#Load Data
setwd("D:/ComicandSonsProductions/GameJam1/SpeakerRecognition/TestData")
VoiceData = read.csv(file = "Test.csv",sep=",")
VoiceData[1]=NULL
names(VoiceData)[1]="Voice"
class(VoiceData$Voice)
VoiceData$Voice = as.factor(VoiceData$Voice)
nrow(VoiceData)
# Split data to reduce duration of computation
training.samples <- VoiceData$Voice %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- VoiceData[training.samples, ]
test.data <- VoiceData[-training.samples, ]
model <- neuralnet(Voice ~., data =  train.data,hidden = c(48, 26, 16), linear.output = FALSE, lifesign = 'full', rep=3, stepmax = 100000, threshold = 0.01)
test_predict <- as.data.frame(predict(model, newdata = test.data, type='response'))
train_predict <- as.data.frame(predict(model, data = train.data, type='response'))
EvaluationData<-data.frame(test.data$Voice,test_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
EvaluationData<-data.frame(train.data$Voice,train_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
EvaluationData
test_predict <- as.data.frame(predict(model, newdata = test.data, type='response'))
test_predict
