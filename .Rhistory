if(!require("osmdata")) install.packages("osmdata")
library(osmdata)
citation ("osmdata")
#The sf we will need to make geometrical calculations.
if(!require("sf")) install.packages("sf")
library(sf)
#Further we need to access tidyverse.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
city="Berlin"
Brandenburg_Gate=c(13.377336846520663,52.516264818429924)
#As second we build a query asking for traffic signals in Berlin.
q <- getbb(city) %>%
opq() %>%
add_osm_feature("highway", "traffic_signals")
#Read the osm data format as a list in R.
signals <- osmdata_sf(q)
#If you access signals:
signals
distances=c(1:length(signals$osm_points$osm_id))
for(i in 1:length(distances)){
distances[i]=distm(Brandenburg_Gate, c(signals$osm_points$geometry[[i]][1],signals$osm_points$geometry[[i]][2]), fun=distGeo)
}
#This is a script for a tutorial
#You can learn to get the coordinates of points of interested by collecting data via open street map.
#For that purpose we will use the osmdata package.
if(!require("osmdata")) install.packages("osmdata")
library(osmdata)
#Do not forget to give credit to the creators.
citation ("osmdata")
#The sf we will need to make geometrical calculations.
if(!require("sf")) install.packages("sf")
library(sf)
#Further we need to access tidyverse.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
if(!require("geosphere")) install.packages("geosphere")
library(geosphere)#package for calculating distance using longitude and latitude
citation ("osmdata")
#The sf we will need to make geometrical calculations.
if(!require("sf")) install.packages("sf")
library(sf)
#Further we need to access tidyverse.
if(!require("tidyverse")) install.packages("tidyverse")
library(tidyverse)
if(!require("geosphere")) install.packages("geosphere")
library(geosphere)#package for calculating distance using longitude and latitude
#First we determine which city we want to study.
city="Berlin"
#Or you use another data source.
Brandenburg_Gate=c(13.377336846520663,52.516264818429924)
q <- getbb(city) %>%
opq() %>%
add_osm_feature("highway", "traffic_signals")
signals <- osmdata_sf(q)
signals
signals$osm_points$geometry
distances=c(1:length(signals$osm_points$osm_id))
for(i in 1:length(distances)){
distances[i]=distm(Brandenburg_Gate, c(signals$osm_points$geometry[[i]][1],signals$osm_points$geometry[[i]][2]), fun=distGeo)
}
min(distances)
sum(distances < 1000)
q2 <- getbb(city) %>%
opq() %>%
add_osm_feature("highway", "primary")
primary <- osmdata_sf(q2)
#Since now we are handeling street, we are not longer interested in osm_points but osm_lines
Lines_primary = st_transform(primary$osm_lines$geometry,4269)
#We need to convert our point at Brandenburg Gate to another data format
POINT_Brandenburg_Gate = as.data.frame(rbind(Brandenburg_Gate))
names(POINT_Brandenburg_Gate)[1]="long1"
names(POINT_Brandenburg_Gate)[2]="lat1"
POINT_Brandenburg_Gate = st_as_sf(POINT_Brandenburg_Gate, coords = c("long1","lat1"))
POINT_Brandenburg_Gate <- st_set_crs(POINT_Brandenburg_Gate, 4269)
#now we can use the st_distance() function to calculate each distance from our Point to each line in our primary street network.
#The smallest distance is:
min(st_distance(POINT_Brandenburg_Gate$geometry, Lines_primary))
library(lubridate)
library(dplyr)
library(geosphere)#package for calculating distance using longitude and latitude
#Clean up memory
rm(list=ls())
#Source storage location (outside the GitHub Repository)
#Because of file size limitation
#files about 100 MB have to be excluded
#D:\STUDIUM\MÃ¼nster\7. Semester\Masterarbeit Daten\Bochum
setwd("D:/STUDIUM/MÃ¼nster/7. Semester/Masterarbeit Daten/Darmstadt")
#Clean up memory
rm(list=ls())
#Voice/Person Recognition: Support Vector Machines
#Author: Max Weinhold
#In order to create a SVR model with R you will need the package e1071
if(!require("e1071")) install.packages("e1071")
library(e1071)
library(tidyverse)
library(sandwich)
library(caret)
#Clean up memory
rm(list=ls())
Training = c(1:12)
Testing = c(1:12)
for(a in 1:12){
print(a)
#Load Data
setwd("D:/ComicandSonsProductions/GameJam1/SpeakerRecognition/TestData")
if(a==1){VoiceData = read.csv(file = "Frequencies12.csv",sep=",")}
if(a==2){VoiceData = read.csv(file = "Frequencies24.csv",sep=",")}
if(a==3){VoiceData = read.csv(file = "Frequencies36.csv",sep=",")}
if(a==4){VoiceData = read.csv(file = "Frequencies48.csv",sep=",")}
if(a==5){VoiceData = read.csv(file = "Frequencies60.csv",sep=",")}
if(a==6){VoiceData = read.csv(file = "Frequencies90.csv",sep=",")}
if(a==7){VoiceData = read.csv(file = "Frequencies120.csv",sep=",")}
if(a==8){VoiceData = read.csv(file = "Frequencies150.csv",sep=",")}
if(a==9){VoiceData = read.csv(file = "Frequencies200.csv",sep=",")}
if(a==10){VoiceData = read.csv(file = "Frequencies300.csv",sep=",")}
if(a==11){VoiceData = read.csv(file = "Frequencies400.csv",sep=",")}
if(a==12){VoiceData = read.csv(file = "Frequencies500.csv",sep=",")}
VoiceData[1]=NULL
names(VoiceData)[1]="Voice"
class(VoiceData$Voice)
VoiceData$Voice = as.factor(VoiceData$Voice)
nrow(VoiceData)
ncol(VoiceData)
print(levels(VoiceData$Voice))
#set.seed(2023)
#trainHits = c(1:10)
#testHits = c(1:10)
#for(b in 1:10){
#training.samples <- VoiceData$Voice %>%
#  createDataPartition(p = 0.8, list = FALSE)
#train.data  <- VoiceData[training.samples, ]
#test.data <- VoiceData[-training.samples, ]
#model <- svm(Voice ~., data =  train.data, type = "nu-classification", kernel = "radial", epsilon = 0.1, nu = 0.0075, tolerance = 0.001, shrinking = TRUE)
#test_predict <- as.data.frame(predict(model, newdata = test.data, type='response'))
#train_predict <- as.data.frame(predict(model, data = train.data, type='response'))
#EvaluationData<-data.frame(test.data$Voice,test_predict)
#names(EvaluationData)<-c("Observation","Prediction")
#EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
#testHits[b] = sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
#EvaluationData<-data.frame(train.data$Voice,train_predict)
#names(EvaluationData)<-c("Observation","Prediction")
#EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
#trainHits[b] = sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
#}
#Training[a] = mean(trainHits)
#Testing[a] = mean(testHits)
}
visualData=as.data.frame(cbind(c(12,24,36,48,60,90,120,150,200,300,400,500),Testing))
plot(c(12,24,36,48,60,90,120,150,200,300,400,500),Testing)
plot = ggplot(data=visualData)+
geom_line(aes(x=V1, y=Testing), size = 2) +
labs(y = "Correct Predictions in %"
, x = "Number of Frequencies"
, title = "Support Vector Machine Performance") +
theme_bw() +
theme(legend.text=element_text(size=12)) +
theme(axis.text=element_text(size=12),axis.title=element_text(size=14,face="bold"))
print(plot)
setwd("D:/ComicandSonsProductions/GameJam1/SpeakerRecognition")
png(file="Plot_SVM_FreqcuencyValidation.png",width=800, height=800)
plot
dev.off()
#Voice/Person Recognition: Support Vector Machines
#Author: Max Weinhold
#In order to create a SVR model with R you will need the package e1071
if(!require("e1071")) install.packages("e1071")
library(e1071)
library(tidyverse)
library(sandwich)
library(caret)
#Clean up memory
rm(list=ls())
Training = c(1:12)
Testing = c(1:12)
for(a in 1:12){
print(a)
#Load Data
setwd("D:/ComicandSonsProductions/GameJam1/SpeakerRecognition/TestData")
if(a==1){VoiceData = read.csv(file = "Frequencies12.csv",sep=",")}
if(a==2){VoiceData = read.csv(file = "Frequencies24.csv",sep=",")}
if(a==3){VoiceData = read.csv(file = "Frequencies36.csv",sep=",")}
if(a==4){VoiceData = read.csv(file = "Frequencies48.csv",sep=",")}
if(a==5){VoiceData = read.csv(file = "Frequencies60.csv",sep=",")}
if(a==6){VoiceData = read.csv(file = "Frequencies90.csv",sep=",")}
if(a==7){VoiceData = read.csv(file = "Frequencies120.csv",sep=",")}
if(a==8){VoiceData = read.csv(file = "Frequencies150.csv",sep=",")}
if(a==9){VoiceData = read.csv(file = "Frequencies200.csv",sep=",")}
if(a==10){VoiceData = read.csv(file = "Frequencies300.csv",sep=",")}
if(a==11){VoiceData = read.csv(file = "Frequencies400.csv",sep=",")}
if(a==12){VoiceData = read.csv(file = "Frequencies500.csv",sep=",")}
VoiceData[1]=NULL
names(VoiceData)[1]="Voice"
class(VoiceData$Voice)
VoiceData$Voice = as.factor(VoiceData$Voice)
nrow(VoiceData)
ncol(VoiceData)
print(levels(VoiceData$Voice))
set.seed(2023)
trainHits = c(1:10)
testHits = c(1:10)
for(b in 1:10){
training.samples <- VoiceData$Voice %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- VoiceData[training.samples, ]
test.data <- VoiceData[-training.samples, ]
model <- svm(Voice ~., data =  train.data, type = "nu-classification", kernel = "radial", epsilon = 0.1, nu = 0.0075, tolerance = 0.001, shrinking = TRUE)
test_predict <- as.data.frame(predict(model, newdata = test.data, type='response'))
train_predict <- as.data.frame(predict(model, data = train.data, type='response'))
EvaluationData<-data.frame(test.data$Voice,test_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
testHits[b] = sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
EvaluationData<-data.frame(train.data$Voice,train_predict)
names(EvaluationData)<-c("Observation","Prediction")
EvaluationData$Accuracy = EvaluationData$Prediction == EvaluationData$Observation
trainHits[b] = sum(EvaluationData$Prediction == EvaluationData$Observation)/nrow(EvaluationData)*100
}
Training[a] = mean(trainHits)
Testing[a] = mean(testHits)
}
visualData=as.data.frame(cbind(c(12,24,36,48,60,90,120,150,200,300,400,500),Testing))
plot(c(12,24,36,48,60,90,120,150,200,300,400,500),Testing)
plot = ggplot(data=visualData)+
geom_line(aes(x=V1, y=Testing), size = 2) +
labs(y = "Correct Predictions in %"
, x = "Number of Frequencies"
, title = "Support Vector Machine Performance") +
theme_bw() +
theme(legend.text=element_text(size=12)) +
theme(axis.text=element_text(size=12),axis.title=element_text(size=14,face="bold"))
print(plot)
setwd("D:/ComicandSonsProductions/GameJam1/SpeakerRecognition")
png(file="Plot_SVM_FreqcuencyValidation.png",width=800, height=800)
plot
dev.off()
Testing
